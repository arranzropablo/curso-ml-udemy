{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arboles de decision\n",
    "\n",
    "Arbol formado por nodos unidos por ramas a partir del nodo raiz.<br>\n",
    "Los últimos nodos del árbol serán los nodos hoja.<br>\n",
    "Cada nodo representa una variable y cada rama un valor posible de esa variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropia\n",
    "\n",
    "Indica la heterogeneidad.\n",
    "\n",
    "$$H(S) = \\sum_{i=1}^{n}{-p_ilog_2(p_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ganancia de información\n",
    "\n",
    "Ganancia de la entropía al añadir una variable (La que tenga mayor ganancia será el nodo que usaremos de raíz y despues cada subnodo del mismo modo).\n",
    "\n",
    "$$ \\Delta H (S, V) = H(S) - \\sum_{c\\in V}{\\frac{\\lvert V = c\\rvert}{\\lvert V \\rvert}H(V = c)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo ID3\n",
    "\n",
    "1. Calculamos la entropía inicial del sistema con la variable a predecir.\n",
    "2. Calculamos la ganancia de información para cada variable posible a añadir.\n",
    "3. Añadimos ese nodo como nodo raiz y repetimos el proceso para añadir nodos hijo en cada rama hasta que todos los posibles datos queden clasificados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Además del ID3 hay otras maneras de saber que variable hay que elegir como subnodo:\n",
    "* Indice de Gini (Se suele utilizar si la variable objetivo a predecir es binaria)\n",
    "* CHAID (Detector automático de la interacción con chi cuadrado)\n",
    "* Reducción de varianza (Cuando la variable a predecir es continua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice de Gini\n",
    "\n",
    "Tenemos una variable con n categorías. <br>\n",
    "Dentro de cada categoría habrá un porcentaje de casos que caerán en una clasificación y otro que caerán en la otra.<br>\n",
    "Se suman los cuadrados de los porcentajes.\n",
    "\n",
    "$$ Gini_{C_1} = (0.2)^2 + (0.8)^2 = 0.68$$\n",
    "$$Gini_{C_2} = (0.6)^2 + (0.4)^2 = 0.52 $$\n",
    "\n",
    "Se suman los valores multiplicandolor por el porcentaje de casos de cada categoría, por ejemplo si hay 10 casos en cada categoría:\n",
    "$$ GiniIndex = \\frac{10}{20}\\cdot 0.68 + \\frac{10}{20}\\cdot 0.52 = 0.60$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de la varianza\n",
    "\n",
    "Lo mismo pero la clasificación en vez de binaria es continua.<br>\n",
    "Calculamos la media y varianza de los totales.<br>\n",
    "Calculamos la media y varianza de cada categoría.<br>\n",
    "Calculamos la varianza ponderada de las dos categorías multiplicandolo por el porcentaje de elementos que caen en esa categoría como en el anterior caso.<br>\n",
    "La variable con menor varianza ponderada será la elegida como nodo de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poda del árbol\n",
    "\n",
    "* Reducción del error en la poda: Tomamos todas y cada una de las hojas y reemplazamos cada nodo por la categoría \"más popular\"\n",
    "* Poda del coste de complejidad: Se trata de ir generando subconjuntos de árboles a partir del original con podas (Siendo $T_1$ el árbol original y $T_n$ un arbol con solo el nodo raíz).<br>\n",
    "    El subarbol a ser reemplazado se elige del siguiente modo:\n",
    "    * Se define la tasa de error para un árbol T como err(T), el número de nodos hoja como leaves(T) y un subárbol como resultado de podar t de T como prune(T, t).\n",
    "    * Definimos M como:\n",
    "    $$ M = \\frac{err(prune(T, t)) - err(T)}{\\lvert leaves(T) \\rvert - \\lvert leaves(prune(T, t))\\rvert}$$\n",
    "    * Calculamos M para todos los posibles subárboles de T y el que minimice el valor de M será eliminado del árbol original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas en los árboles\n",
    "\n",
    "### Variables continuas\n",
    "\n",
    "En algunos puntos del algoritmo necesitamos si o si que las variables sean categóricas.<br>\n",
    "Tendremos que definir unos thresholds para que la variable continua se divida en categorías en función de en que rango caiga.<br>\n",
    "Para esto:\n",
    "* Ordenamos el dataset en orden ascendente de la variable continua que queremos pasar a discreta.\n",
    "* Calculamos el punto medio de cada cambio. Es decir, la media entre los valores de la variable de una categoría a la otra y los rangos estarán delimitados por estos puntos o thresholds.\n",
    "\n",
    "### Faltan datos\n",
    "\n",
    "Si falta un dato podemos: \n",
    "* Asignar el valor más frecuente de la columna.\n",
    "* Asignar el valor más frecuente de la columna en función de la variable objetivo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
